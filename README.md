#  Fine-Tuning DeepSeek-R1 for Medical Reasoning with Unsloth

This project fine-tunes the `DeepSeek-R1-Distill-Llama-8B` language model using the [Unsloth](https://github.com/unslothai/unsloth) library to improve performance on **medical reasoning** tasks. It can be adapted for legal reasoning and other domain-specific tasks like clinical decision support or question generation.

---

##  Features

-  Uses **LoRA** (Low-Rank Adaptation) for efficient fine-tuning
-  Supports **4-bit quantization** for memory efficiency
-  Trained on the [medical-o1-reasoning-SFT] dataset
-  Includes structured medical prompts and Chain-of-Thought (CoT) reasoning
-  Integrated with **Weights & Biases** and **Hugging Face Hub**

---

##  Tech Stack

- Python
- ğŸ¤— Transformers
- ğŸ¦¥ Unsloth
- ğŸ§  DeepSeek-R1 (Distilled LLama 8B)
- ğŸ©º Medical CoT Dataset
- ğŸ§ª Weights & Biases for tracking
- ğŸ—ƒï¸ Hugging Face Hub for model storage

---

## ğŸ“¦ Installation

Install the required packages:

pip install unsloth
pip install --force-reinstall --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git
pip install bitsandbytes triton wandb

