# ğŸ§  Fine-Tuning DeepSeek-R1 for Medical Reasoning with Unsloth

This project fine-tunes the `DeepSeek-R1-Distill-Llama-8B` language model using the [Unsloth](https://github.com/unslothai/unsloth) library to improve performance on **medical reasoning** tasks. It can be adapted for legal reasoning and other domain-specific tasks like clinical decision support or question generation.

---

## ğŸš€ Features

- âœ… Uses LoRA (Low-Rank Adaptation) for efficient fine-tuning
- âœ… Supports **4-bit quantization** for memory efficiency
- âœ… Trained on the [medical-o1-reasoning-SFT](https://huggingface.co/datasets/FreedomIntelligence/medical-o1-reasoning-SFT) dataset
- âœ… Includes structured medical prompts and Chain-of-Thought (CoT) reasoning
- âœ… Integrated with **Weights & Biases** and **Hugging Face Hub**

---

## ğŸ§° Tech Stack

- Python
- ğŸ¤— Transformers
- ğŸ¦¥ Unsloth
- ğŸ§  DeepSeek-R1 (Distilled LLama 8B)
- ğŸ©º Medical CoT Dataset
- ğŸ§ª Weights & Biases for tracking
- ğŸ—ƒï¸ Hugging Face Hub for model storage

---

## ğŸ“¦ Installation

Install the required packages:

pip install unsloth
pip install --force-reinstall --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git
pip install bitsandbytes triton wandb

